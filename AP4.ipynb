{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "\n",
    "## Item a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, precisamos passar essa equação para uma forma polinomial, de modo que consigamos trabalhar adequadamente com mínimos quadrados. Para isso, utilizemos do logaritmo natural:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P &= bL^\\alpha K^{1-\\alpha} \\\\\n",
    "ln(P) &= ln(bL^\\alpha K^{1-\\alpha}) \\\\\n",
    "ln(P) &= ln(b) + ln(L^\\alpha) + ln(K^{1-\\alpha}) \\\\\n",
    "ln(P) &= ln(b) + \\alpha ln(L) + (1-\\alpha)ln(K) \\\\\n",
    "ln(P) &= ln(b) + \\alpha ln(L) + ln(K) - \\alpha ln(K) \\\\\n",
    "ln(P) &= ln(K) + ln(b) + \\alpha(ln(L) - ln(K)) \\\\\n",
    "ln(P) - ln(K) &= ln(b) + \\alpha(ln(L/K)) \\\\\n",
    "ln(P/K) &= ln(b) + \\alpha(ln(L/K))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, obtemos uma equação da forma $y = ax + b$, que podemos facilmente resolver por mínimos quadrados.\n",
    "\n",
    "Agora, vamos criar vetores com os nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [100; 101; 112; 122; 124; 122; 143; 152; 151; 126; 155; 159; 153; 177; 184; 169; 189; 225; 227; 223; 218; 231; 179; 240];\n",
    "L = [100; 105; 110; 117; 122; 121; 125; 134; 140; 123; 143; 147; 148; 155; 156; 152; 156; 183; 198; 201; 196; 194; 146; 161];\n",
    "K = [100; 107; 114; 122; 131; 138; 149; 163; 176; 185; 198; 208; 216; 226; 236; 244; 266; 298; 335; 366; 387; 407; 417; 431];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, montemos a matriz $A$ e o vetor $b$ do nosso método de mínimos quadrados com base na equação encontrada anteriormente. A matriz $A$ será formada por uma coluna de 1s, que será multiplicada pelo $ln(b)$, e pelo vetor $ln(L/K)$, que será multiplicado pelo $\\alpha$, e o vetor $b$ será o vetor $ln(P/K)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \"A\"\n",
      "\n",
      "   1.   0.       \n",
      "   1.  -0.0188685\n",
      "   1.  -0.0357181\n",
      "   1.  -0.0418471\n",
      "   1.  -0.0711763\n",
      "\n",
      "  \"b\"\n",
      "\n",
      "   0.\n",
      "  -0.0577083\n",
      "  -0.0176996\n",
      "   0.\n",
      "  -0.0549158"
     ]
    }
   ],
   "source": [
    "A = [ones(P) log(L ./ K)];\n",
    "disp(\"A\", A(1:5, :))\n",
    "\n",
    "b = log(P ./ K);\n",
    "disp(\"b\", b(1:5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, precisamos resolver o sistema $A^TA\\overline{x} = A^Tb$. Para isso, utilizaremos nossa clássica `Gaussian_Elimination_4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [x, C, P]=Gaussian_Elimination_4(A, b)\n",
    "    C=[A,b];\n",
    "    [n]=size(C,1);\n",
    "    // Inicializando a matriz P\n",
    "    P = eye(n,n)\n",
    "    for j=1:(n-1)\n",
    "        // Se o pivô não for o maior valor de sua coluna, troca as linhas\n",
    "        if max(abs(C(j:n,j))) ~= abs(C(j,j)) then\n",
    "            // Encontrando a linha com o maior pivô em módulo\n",
    "            moduled_vector = abs(C(j:n,j))\n",
    "            max_pivot_index = find(moduled_vector == max(moduled_vector))\n",
    "            // Trocando essas linhas na C e na P\n",
    "            C([j j+max_pivot_index-1],:) = C([j+max_pivot_index-1 j],:)\n",
    "            P([j j+max_pivot_index-1],:) = P([j+max_pivot_index-1 j],:)\n",
    "        end\n",
    "        //O pivô está na posição (j,j)\n",
    "        for i=(j+1):n\n",
    "            //O elemento C(i,j) é o elemento na posição (i,j) of L na decomposição LU de A\n",
    "            C(i,j)=C(i,j)/C(j,j);\n",
    "            //Linha i  Linha i - C(i,j)*Linha j\n",
    "            //Somente os elementos da diagonal ou acima dela são computados\n",
    "            //(aqueles que compõem a matrix U)\n",
    "            C(i,j+1:n+1)=C(i,j+1:n+1)-C(i,j)*C(j,j+1:n+1);\n",
    "        end\n",
    "    end\n",
    "    x=zeros(n,1);\n",
    "    // Calcula x, sendo Ux=C(1:n,n+1)\n",
    "    x(n)=C(n,n+1)/C(n,n);\n",
    "    for i=n-1:-1:1\n",
    "        x(i)=(C(i,n+1)-C(i,i+1:n)*x(i+1:n))/C(i,i);\n",
    "    end\n",
    "    C=C(1:n,1:n);\n",
    "endfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, resolvendo o sistema, obtemos o seguinte vetor $\\overline{x}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_barra  = \n",
      "   0.0070440\n",
      "   0.7446062"
     ]
    }
   ],
   "source": [
    "x_barra = Gaussian_Elimination_4(A'*A, A'*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, apesar do segundo elemento ser justamente o $\\alpha$ que estávamos procurando, o primeiro ainda é o logaritmo natural de $b$. Então, calculemos o verdadeiro $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " alpha  = \n",
      "   0.7446062\n",
      " b  = \n",
      "   1.0070689"
     ]
    }
   ],
   "source": [
    "alpha = x_barra(2)\n",
    "\n",
    "e = %e;\n",
    "b = e^(x_barra(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, obtemos assim a nossa estimativa da equação de Cobb-Douglas:\n",
    "\n",
    "$$\n",
    "P = 1.0070689 \\cdot L^{0.7446062} \\cdot K^{0.2553938}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item b)\n",
    "\n",
    "Para calcular nossas previsões para 1910 e 1920, vamos definir uma função para a equação de Cobb-Douglas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Função de Cobb-Douglas\n",
    "function [P] = Cobb_Douglas(L, K)\n",
    "    // Calculando P com os parâmetros encontrados\n",
    "    P = 1.0070689 * L^(0.7446062) * K^(1 - 0.7446062)\n",
    "endfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desse modo, façamos nossas estimativas utilizando os dados fornecidos na tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P_1910  = \n",
      "   161.76185\n",
      " P_1920  = \n",
      "   236.07216"
     ]
    }
   ],
   "source": [
    "P_1910 = Cobb_Douglas(147, 208)\n",
    "P_1920 = Cobb_Douglas(194, 407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para 1910, obtivemos uma previsão de aproximadamente 162, enquanto o dado real é de 159. E, para 1920, obtivemos 236 quando o verdadeiro valor foi de 231. Portanto, pode-se dizer que foram boas estimativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, vamos carregar e organizar nossos dados, separando-os em features e targets de treino e de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = csvRead(\"cancer_train_2024.csv\", \";\");\n",
    "data_test = csvRead(\"cancer_test_2024.csv\", \";\");\n",
    "\n",
    "X_train = data_train(:, 1:10);\n",
    "X_test = data_test(:, 1:10);\n",
    "y_train = data_train(:, 11);\n",
    "y_test = data_test(:, 11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de calcularmos nosso modelo, primeiro precisamos adicionar uma coluna de 1s tanto no conjunto de treino quanto no de teste, referente ao bias dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \"X_train_bias\"\n",
      "\n",
      "   1.   0.64     0.2643   0.6515   0.4006   0.8182   0.8037   0.7031   0.7311   0.7957   0.8078\n",
      "   1.   0.7318   0.4524   0.705    0.5306   0.5856   0.2277   0.2036   0.3488   0.5961   0.5816\n",
      "   1.   0.7005   0.541    0.6897   0.4814   0.7574   0.4629   0.4625   0.6357   0.6806   0.6157\n",
      "   1.   0.4063   0.5188   0.4116   0.1545   0.9848   0.8219   0.5656   0.5229   0.8543   1.    \n",
      "   1.   0.4429   0.3997   0.438    0.1909   0.8832   0.4922   0.3697   0.402    0.6865   0.7813\n",
      "\n",
      "  \"X_test_bias\"\n",
      "\n",
      "   1.   0.7123   0.6152   0.6929   0.4866   0.7038   0.6374   0.6044   0.5551   0.6975   0.6843\n",
      "   1.   0.4544   0.6475   0.4303   0.1884   0.5172   0.3936   0.1879   0.162    0.6933   0.6526\n",
      "   1.   0.7327   0.7767   0.7207   0.4986   0.661    0.7135   0.6281   0.6691   0.8754   0.7801\n",
      "   1.   0.3826   0.6058   0.3577   0.1337   0.6536   0.2592   0.0632   0.093    0.5769   0.6893\n",
      "   1.   0.4179   0.5911   0.3937   0.1612   0.5418   0.2987   0.092    0.0785   0.5492   0.652 "
     ]
    }
   ],
   "source": [
    "X_train_bias = [ones(y_train) X_train];\n",
    "disp(\"X_train_bias\", X_train_bias(1:5, :))\n",
    "\n",
    "X_test_bias = [ones(y_test) X_test];\n",
    "disp(\"X_test_bias\", X_test_bias(1:5, :))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos utilizar o método dos mínimos quadrados com o conjunto de treino para aprender nosso modelo. Para isso, utilizaremos nossa função `Gaussian_Elimination_4` para resolver o sistema $A^TA\\overline{x} = A^Tb$, onde $A$ é o nosso `X_train_bias` e $b$ é o nosso `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_barra  = \n",
      "  -6.2101493\n",
      "   15.902409\n",
      "   1.5568757\n",
      "  -5.0718598\n",
      "  -7.1846562\n",
      "   1.2702227\n",
      "  -0.9298812\n",
      "   0.5285964\n",
      "   1.9535131\n",
      "  -0.0470564\n",
      "   0.7701829"
     ]
    }
   ],
   "source": [
    "x_barra = Gaussian_Elimination_4(X_train_bias'*X_train_bias, X_train_bias'*y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito, temos nossos parâmetros, ou seja, os coeficientes do nosso hiperplano! Com ele, podemos agora ver como ele se ajustou aos dados de treinamento. Para isso, calculemos primeiro as previsões para esse conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ans  =\n",
      "   0.8729158\n",
      "   0.4847284\n",
      "   1.2751264\n",
      "   0.3981943\n",
      "   0.0766325"
     ]
    }
   ],
   "source": [
    "y_train_pred = X_train_bias * x_barra;\n",
    "y_train_pred(1:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, confiramos nossas previsões para o conjunto de treinamento. Se multiplicarmos elemento a elemento nossa previsão com os rótulos verdadeiros, valores positivos indicarão previsões corretas (previsão e rótulo positivos ou previsão e rótulo negativos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_accuracy  = \n",
      "   0.9214286"
     ]
    }
   ],
   "source": [
    "y_train_pred_conf = y_train_pred .* y_train;\n",
    "train_accuracy = sum(y_train_pred_conf > 0)/size(y_train)(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, nosso modelo obteve uma acurácia de 92% no conjunto de treinamento. Agora, vamos fazer o mesmo processo para o conjunto de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ans  =\n",
      "   1.2639506\n",
      "  -0.3352808\n",
      "   1.7880925\n",
      "  -0.6495086\n",
      "  -0.7104726"
     ]
    }
   ],
   "source": [
    "y_test_pred = X_test_bias * x_barra;\n",
    "y_test_pred(1:5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test_accuracy  = \n",
      "   0.8892857"
     ]
    }
   ],
   "source": [
    "y_test_pred_conf = y_test_pred .* y_test;\n",
    "test_accuracy = sum(y_test_pred_conf > 0)/size(y_test)(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tivemos uma acurácia de aproximadamente 89% no conjunto de teste, o que é bem razoável para um modelo tão simples como esse.\n",
    "\n",
    "Por fim, vamos calcular a matriz de confusão e suas medidas decorrentes para o conjunto de teste. Para isso, primeiro precisamos calcular o número de previsões de cada categoria: verdadeiro positivo (TP), falso positivo (FP), verdadeiro negativo (TN) e falso negativo (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " confusion_matrix  = \n",
      "   80.   25. \n",
      "   6.    169."
     ]
    }
   ],
   "source": [
    "TP = sum((y_test_pred >= 0) .* (y_test >= 0));\n",
    "FP = sum((y_test_pred >= 0) .* (y_test < 0));\n",
    "TN = sum((y_test_pred < 0) .* (y_test < 0));\n",
    "FN = sum((y_test_pred < 0) .* (y_test >= 0));\n",
    "\n",
    "confusion_matrix = [TP FP; FN TN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos calcular algumas medidas interessantes resultantes dessa matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy  = \n",
      "   0.8892857\n",
      " precision  = \n",
      "   0.7619048\n",
      " recall  = \n",
      "   0.9302326\n",
      " fpr  = \n",
      "   0.1288660\n",
      " fnr  = \n",
      "   0.0697674"
     ]
    }
   ],
   "source": [
    "accuracy = (TP + TN)/(TP + FP + FN + TN)\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "fpr = FP/(FP + TN)\n",
    "fnr = FN/(FN + TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisemos essas medidas:\n",
    "\n",
    "- A **acurácia** é a proporção de registros classificados corretamente, sejam eles positivos ou negativos. Assim, pela nossa medida, nosso modelo acerta a previsão em 89% das vezes.\n",
    "- A **precisão** é a proporção de registros classificados como positivos que são realmente positivos. Dessa forma, se nosso modelo diz que uma pessoa tem câncer de mama, a chance de ela realmente ter é de 76%.\n",
    "- A **revocação** ou **recall** é a proporção de registros positivos que são classificados como positivos. Desse modo, a probabilidade de uma pessoa com câncer de mama ser detectada pelo nosso modelo é de 93%.\n",
    "- A **probabilidade de falso alarme** ou **false positive rate (FPR)** é a chance de um registro negativo ser classificado como positivo. Assim, uma pessoa que não tem câncer de mama tem 13% de chance de receber diagnóstico positivo por esse modelo.\n",
    "- E a **probabilidade de falsa omissão de alarme** ou **false negative rate (FNR)** é a probabilidade de um registro positivo ser classificado como negativo. Dessa forma, uma pessoa com câncer de mama tem 7% de chance de ser diagnosticada como não tendo pelo nosso modelo.\n",
    "\n",
    "Uma análise interessante é que há um trade-off entre a precisão e a revocação (recall), ou seja, quanto maior um, menor o outro. No nosso caso, nossa revocação é alta enquanto nossa precisão é baixa, e isso é bom! Nesse contexto, é muito menos prejudicial classificar incorretamente um negativo como positivo do que um positivo como negativo. Uma pessoa que não tem câncer mas que foi classificada como tendo câncer vai apenas fazer outros exames para conferência e eventualmente descobrirá que na verdade não tinha. Por outro lado, uma pessoa que tem câncer e que foi classificada como não tendo câncer possivelmente não fará exames de garantia e terá o estado de sua doença piorado, podendo levar até a uma situação irreversível. Portanto, nosso modelo acabou escolhendo certo qual das duas métricas manter superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3\n",
    "\n",
    "Podemos tirar algumas conclusões com base no vetor de parâmetros `x_barra`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_barra  = \n",
      "  -6.2101493\n",
      "   15.902409\n",
      "   1.5568757\n",
      "  -5.0718598\n",
      "  -7.1846562\n",
      "   1.2702227\n",
      "  -0.9298812\n",
      "   0.5285964\n",
      "   1.9535131\n",
      "  -0.0470564\n",
      "   0.7701829"
     ]
    }
   ],
   "source": [
    "x_barra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada um desses valores refere-se a uma das características do nosso dado (exceto o primeiro, que refere-se ao bias). Assim, podemos observar que algumas variáveis, como a primeira (referente ao 2° elemento do vetor) e a quarta (referente ao 5° elemento do vetor) possuem um peso maior na previsão dos registros. Em contrapartida, variáveis como a sétima e a nona têm pouca influência no resultado. Dessa forma, vamos tentar eliminar essas variáveis uma de cada vez para ver como o desempenho do modelo se comporta em vista de sua simplificação.\n",
    "\n",
    "Primeiramente, eliminemos a penúltima variável, que possui o menor parâmetro relacionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_barra_one_less  = \n",
      "  -6.2101493\n",
      "   15.902409\n",
      "   1.5568757\n",
      "  -5.0718598\n",
      "  -7.1846562\n",
      "   1.2702227\n",
      "  -0.9298812\n",
      "   0.5285964\n",
      "   1.9535131\n",
      "   0.7701829"
     ]
    }
   ],
   "source": [
    "X_test_one_less = [X_test_bias(:, 1:9) X_test_bias(:, 11)];\n",
    "x_barra_one_less = x_barra([1:9, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos fazer a nova previsão e calcular sua acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test_one_less_accuracy  = \n",
      "   0.8678571"
     ]
    }
   ],
   "source": [
    "y_test_pred_one_less = X_test_one_less * x_barra_one_less;\n",
    "y_test_pred_one_less_conf = y_test_pred_one_less .* y_test;\n",
    "test_one_less_accuracy = sum(y_test_pred_one_less_conf > 0)/size(y_test)(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tivemos uma perda de aproximadamente 2% de acurácia, o que é bem razoável! Eliminar aquela variável não surtiu tanto efeito na previsão.\n",
    "\n",
    "Para finalizar, vamos fazer isso em loop até não sobrar nenhuma variável no modelo (até termos apenas o bias). Abaixo está o código do loop e as acurácias ao longo da remoção das variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracies  = \n",
      "   0.8892857\n",
      "   0.8678571\n",
      "   0.9\n",
      "   0.8857143\n",
      "   0.9071429\n",
      "   0.8142857\n",
      "   0.6964286\n",
      "   0.6928571\n",
      "   0.7964286\n",
      "   0.425\n",
      "   0.6928571"
     ]
    }
   ],
   "source": [
    "// Vetor para armazenar a acurácia de cada teste\n",
    "accuracies = [test_accuracy; zeros((10, 1))];\n",
    "\n",
    "// Inicializando os dados e o vetor de pesos reduzidos\n",
    "reduced_X_test = X_test_bias;\n",
    "reduced_x_barra = x_barra;\n",
    "\n",
    "// Em cada iteração...\n",
    "for i = 1:10\n",
    "    // Encontra o índice do menor peso em módulo do vetor x_barra\n",
    "    weaker_variable = find(abs(reduced_x_barra) == min(abs(reduced_x_barra(2:(12 - i)))));\n",
    "    // Remove do dado a coluna correspondente a esse peso\n",
    "    reduced_X_test = reduced_X_test(:, [1:(weaker_variable - 1) (weaker_variable + 1):(12 - i)]);\n",
    "    // Remove o próprio peso do vetor\n",
    "    reduced_x_barra = reduced_x_barra([1:(weaker_variable - 1) (weaker_variable + 1):(12 - i)]);\n",
    "\n",
    "    // Faz uma nova predição do conjunto de teste\n",
    "    reduced_y_test_pred = reduced_X_test * reduced_x_barra;\n",
    "    // Calcula a acurácia\n",
    "    reduced_y_test_pred_conf = reduced_y_test_pred .* y_test;\n",
    "    reduced_accuracy = sum(reduced_y_test_pred_conf > 0)/size(y_test)(1);\n",
    "    // Salva-a no vetor\n",
    "    accuracies(i + 1) = reduced_accuracy;\n",
    "end\n",
    "\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível observar que, removendo algumas das variáveis menos relevantes (até 4 delas), a acurácia permanece alta, até mesmo aumentando em alguns casos. Isso ocorre provavelmente pela redução de ruído ocasionada pela remoção dessas características. No entanto, quando temos poucas variáveis restantes, o desempenho começa a diminuir, dado que há poucas informações à disposição para realizar a análise e a predição corretas.\n",
    "\n",
    "Uma informação curiosa é o fato de que o último ponto, referente ao caso em que todas as variáveis foram removidas, restando apenas o bias, possui quase 70% de acurácia, e isso ocorre porque, como o valor do bias é negativo, então todos os registros são classificados como negativos. Assim, dado que quase 70% dos registros são realmente negativos, esse acaba sendo exatamente o valor da acurácia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scilab",
   "language": "scilab",
   "name": "scilab"
  },
  "language_info": {
   "file_extension": ".sci",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scilab",
   "name": "scilab",
   "version": "0.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
